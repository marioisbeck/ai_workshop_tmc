{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Part II</br>Building the AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a4c19-69f0-45ff-93fd-18bc96358d94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e9bdb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) Vaswani et al. (2017, Google Brain/ Research)\n",
    "* 5 days to 1 million users (OpenAI)\n",
    "* 1.8 billion monthly visits in March 2023 (OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cd24b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Agricultural Revolution**: Around 10,000 BCE, shift to settled farming.\n",
    "- **Industrial Revolution**: Late 18th century, rise of industrialization.\n",
    "- **Digital (Computer) Revolution**: Mid-20th century, advent of computers.\n",
    "- **AI Revolution**: Early 21st century, integration of artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f52aeb-e03e-445e-898f-50995dd79148",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae92ca-3e92-4b2f-8b78-10130d41749e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Introduction to LLMs\n",
    "* Starting Docker Containers\n",
    "* Build AI Assistant (Walkthrough & HandsOn)\n",
    "    * Ingestion (Load, Split, Embed, Store)\n",
    "    * Similarity Search\n",
    "    * Combine Context\n",
    "    * Response Generation\n",
    "* Langserve and Streamlit App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192fd6c0-9607-4a12-bccd-f8183153597b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We will build a simple retrieval augmented generation (RAG) pipeline and complete HandsOn tasks.\n",
    "* The notebook is based on [langchains rag intro](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb).\n",
    "* We build towards a broader understanding of the RAG langscape langchain's [rag from scratch](https://github.com/langchain-ai/rag-from-scratch/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2cb58-bb54-4629-a0c9-4e1afe38140b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812f0d9-1fc8-43ba-b72a-5989710ff4f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Complete Installation and Understand Functioning of Essential Tools  \n",
    "2. **Understand the Basics of Large Language Models (LLMs)**\n",
    "3. **Understand on a Programmatic Level how AI Assistants are Built**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836d5ad-e281-4977-8ca7-285fbe1ce20d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting Docker Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064f738-58cc-4365-9800-11a3803f54af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "docker-compose -up -d --build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab14572-84d8-4e92-a999-2f5cabfe9d32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fe82f-a383-4f2a-b68d-1ad408d866bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Slides are shamelessly taken from 3Blue1Brown's [But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning](https://www.youtube.com/watch?v=wjZofJX0v4M&ab_channel=3Blue1Brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b109fa-ec86-42bc-a09a-ac5b79148a5d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![/overview.jpeg](assets/imgs/overview.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ffd28-4838-48d7-bc8c-b5e671b90d1d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![/tokens.jpeg](assets/imgs/tokens.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df524967-2689-49da-b785-545a0abc524a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![/giving_meaning.jpeg](assets/imgs/giving_meaning.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780faa85-325c-45ce-a1a6-2185b16a61b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7374b2-ce57-4511-be6a-e5130984c9e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![assets/imgs/simple_rag.png](assets/imgs/simple_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963535df",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- **Ingestion**: Load and preprocess documents for further processing.\n",
    "    - **Load**: Upload documents to the backend.\n",
    "    - **Split**: Split documents into manageable chunks using characters, sections, semantic meaning, and delimiters.\n",
    "    - **Embed**: Convert document chunks (and query) into vector embeddings for representation.\n",
    "    - **Store**: Store the embeddings in a vector database (Vectorstore) for efficient retrieval.\n",
    "- **Similarity Search**: Use the query embedding to search and retrieve the most relevant document chunks from the Vectorstore.\n",
    "- **Combine Context**: Combine retrieved document chunks with the query to provide context for the generation model.\n",
    "- **Response Generation**: Use a language model to generate a response based on the query and retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434f741",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832b105-edf7-4160-9e76-ac5d351337fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Keep it Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871a327-6b29-4a71-ba29-5cb4392b0fff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following is only to suppress output which we do not care about in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "219cb0fc-1209-4ae5-ba7e-2b3b31119aa2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Setting USER_AGENT variable for jupyter notebook\n",
    "os.environ['USER_AGENT'] = 'jovyan'\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Disable info messages\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb312d-8a07-4df3-8462-72ac526715f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import bs4  # Library for web scraping and parsing HTML/XML\n",
    "from langchain import hub  # Access langchain hub for pre-built tools and models\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Tool to split text recursively by characters\n",
    "from langchain_core.output_parsers import StrOutputParser  # Parses output into strings\n",
    "from langchain_core.runnables import RunnablePassthrough  # Pass-through runnable for data processing\n",
    "from langchain_community.chat_models import ChatOllama  # Chat model from Langchain Community\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings  # Ollama embeddings for text representation\n",
    "from langchain_community.document_loaders import WebBaseLoader  # Load documents from the web\n",
    "from langchain_community.vectorstores import Chroma  # Chroma vector store for efficient retrieval\n",
    "from langchain_community.document_loaders import PyPDFLoader # reading in pdfs\n",
    "from langchain.prompts import ChatPromptTemplate # class for promts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cb8c7-eb50-4a78-a199-e35ee99f70c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21a5073d-13dc-4efd-a678-844268160afa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "OLLAMA_LARGE_LANGUAGE_MODEL = \"wizardlm2:7b\"  # Specifies the large language model version\n",
    "OLLAMA_SERVER = \"http://ollama:11434\"  # URL for the Ollama server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9a0a964e-d7d5-46f3-bc88-645023b6a615",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is the TMC Entrepreneurial Lab?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cae3a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492978da-2e2b-4e99-a399-754e17092e29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952dd1ab-246a-4962-a34a-94ac01ae08eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a72179-477a-4769-b161-563b01ffc6a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "071aa7c5-f6e6-45a0-babb-1bdb364b70b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# pdf document loader\n",
    "loader = PyPDFLoader(\n",
    "    \"./backend/tmc_tel_lab.pdf\"\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8c62ac47-eb7d-4d8e-b620-2d9a61de25e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b43c96b9-4866-43de-8abe-6cacc65c4697",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, dri\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce950f21-f367-416e-a3ee-2e6e112969f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HandsOn --- Web Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac2e60-8327-4fad-8817-55b090073e56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> How can you use the WebBaseLoader to load the contents of the following website: \"https://www.themembercompany.com/nl/employeneurship\"?\n",
    "\n",
    "> How long is the page_content of the resulting document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5778c31a-6138-4130-8865-31a08e82b9fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# # HandsOn: - Web Loader\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://www.themembercompany.com/nl/employeneurship\",)\n",
    "# )\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8f04f1c4-ddf2-4c00-b3dd-8154e3ef994a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# # show character length page content\n",
    "# len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e55bc-acee-4378-9842-a20a504c5e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85118b9-cfed-4e89-9c02-75cd1be5ad1f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Splitter](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad834c-c2a5-4674-881a-663839b5565f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![assets/imgs/splitting_documents.png](assets/imgs/splitting_documents.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e668d339-3951-4662-8387-c3d296646906",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200, \n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca169f93-215a-4a99-bb28-b5b41d62aa08",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46ca07fc-0036-4a60-9912-c7568d60e862",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "359faa46-81d6-4832-a193-5f2ad5009c47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportuniti\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e70dfee8-75c6-4504-87c2-a46673488b8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportunities for growth, and\\nbreakthroughs are celebrated.\\nWhether it's pioneering technology or disruptive healthcare solutions, the Entrepreneurial\\nLab provides the space, resources, and collaborative spirit to turn your vision into a\\nsuccessful venture.\\nHow it works\\nSounds promising, doesn't it? But how exactly does the Entrepreneurial Lab operate?\\n> Our employeneurs work on their own innovation if they desire, but they do this\\nalongside their client project.\\n> TMC supports with a physical lab and possibly even with financial resources.\\n> The ownership of the innovation remains of our employeneurs; TMC has no interest in\\nit.\\nBUILDING THE FUTURE IN\\nThe Entrepreneurial Lab\\nThis is the place where you can let your technical dreams come true. If you\\nhave a groundbreaking idea for a new innovation, you can initiate your\\nproject and take it from concept to reality with the support of a vibrant\\ncommunity of like-minded innovators.\\x00 \\x00\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c580505e-c75d-4749-8c52-b3171654542c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportunities for growth, and\\nbreakthroughs are celebrated.\\nWhether it's pioneering technology or disruptive healthcare solutions, the Entrepreneurial\\nLab provides the space, resources, and collaborative spirit to turn your vision into a\\nsuccessful venture.\\nHow it works\\nSounds promising, doesn't it? But how exactly does the Entrepreneurial Lab operate?\\n> Our employeneurs work on their own innovation if they desire, but they do this\\nalongside their client project.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "238b3a45-8093-4694-8697-6406c5b9c146",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alongside their client project.\\n> TMC supports with a physical lab and possibly even with financial resources.\\n> The ownership of the innovation remains of our employeneurs; TMC has no interest in\\nit.\\nBUILDING THE FUTURE IN\\nThe Entrepreneurial Lab\\nThis is the place where you can let your technical dreams come true. If you\\nhave a groundbreaking idea for a new innovation, you can initiate your\\nproject and take it from concept to reality with the support of a vibrant\\ncommunity of like-minded innovators.\\x00 \\x00'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba267a-bc98-4020-bbdf-38f3e0a3d849",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Text embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615960ac-fe6c-4382-96d2-7daa65d9d358",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14444a47-40d4-4d30-8990-396318d6ddf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "OLLAMA_EMBEDDING_MODEL = \"mxbai-embed-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d6378c5-2836-4a98-8d3d-e1a4aa4ab8e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    "query_result = embedding.embed_query(question)\n",
    "split_result = embedding.embed_query(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "596cfd58-11c9-45d3-94df-52580ab49daa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4f790c8-28ab-404a-8249-09b7b683ffb7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f717689a-9cbd-4259-9ef7-802b09f05440",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6084602475166321,\n",
       " 0.10699772834777832,\n",
       " -0.6222756505012512,\n",
       " -0.09857344627380371]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_result[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe97626-8cf4-4944-bf85-839e38e07745",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) is reccomended (1 indicates identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12668e52-bad0-4b0c-9c12-861e3c5b6c4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2, print_output = False):\n",
    "    \n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    \n",
    "    if print_output:\n",
    "        print(\"Cosine Similarity:\", similarity)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0e464ae-6b39-4fa3-a758-5dc7e1cdc89a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7252169697102998\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(query_result, split_result, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379847f-a71f-4b39-aac8-22afbe0eb092",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HandsOn --- Better Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19f006-cc6f-47a1-a300-16786601cc64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Write code to use the more sophisticated `mxbai-embed-large` instead of the `all-miniml` embedding model with the local Ollama instance. This enables better performance and more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39b29cf5-23ed-4c9c-8bc0-76e64e128cbf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# OLLAMA_EMBEDDING_MODEL = \"all-minilm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db83ad60-8290-4636-b426-fee4c8cde817",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# embedding = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    "# query_result = embedding.embed_query(question)\n",
    "# split_result = embedding.embed_query(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "588c2cf6-4e9c-46c7-89ab-5cfc4f9f13b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# similarity = cosine_similarity(query_result, split_result, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9fdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HandsOn --- Stroopwafel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d587e2c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Similar to the `Japan - Germany` example from the `Introduction to LLMs` we will now calculate the distance between Netherlands and Germany in the vector space. This we can then use to understand what item in Germany corresponds to what the stroopwafel is in the Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abc0e648",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "OLLAMA_EMBEDDING_MODEL = \"mxbai-embed-large\"\n",
    "embedding = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b8cc653",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming embedding is some pre-trained embedding model with an embed_query method\n",
    "words = [\"Bratwurst\", \"Mercedes\", \"Schwarzwälder Kirschtorte\", \"Berliner\", \"Lebkuchen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6e975ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# getting vectors of tokens/ words\n",
    "stroopwafel_embedding = np.array(embedding.embed_query(\"Stroopwafel\"))\n",
    "netherlands_embedding = np.array(embedding.embed_query(\"Netherlands\"))\n",
    "germany_embedding = np.array(embedding.embed_query(\"Germany\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25a9d2d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# calculating the comparison vector\n",
    "comparison_embedding = stroopwafel_embedding - (netherlands_embedding - germany_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0de488df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initiating variables\n",
    "highest_similarity = -1\n",
    "closest_word = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54dd1d9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bratwurst\n",
      "Cosine Similarity: 0.7104238314729429\n",
      "\n",
      "Mercedes\n",
      "Cosine Similarity: 0.5495463777308662\n",
      "\n",
      "Schwarzwälder Kirschtorte\n",
      "Cosine Similarity: 0.6412197961550299\n",
      "\n",
      "Berliner\n",
      "Cosine Similarity: 0.7296874018682962\n",
      "\n",
      "Lebkuchen\n",
      "Cosine Similarity: 0.6988265639666382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running the loop\n",
    "for word in words:\n",
    "    \n",
    "    # embedding the query word\n",
    "    word_embedding = np.array(embedding.embed_query(word))\n",
    "\n",
    "    # generating output\n",
    "    print(word)\n",
    "    similarity = cosine_similarity(comparison_embedding, word_embedding, True)\n",
    "    print(\"\")\n",
    "\n",
    "    # capturing highest similarity\n",
    "    if similarity > highest_similarity:\n",
    "        highest_similarity = similarity\n",
    "        closest_word = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe6757bc-19f4-4e6f-ae27-4e1664a7f049",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word closest to 'stroopwafel' is 'Berliner' with a cosine similarity of 0.7296874018682962.\n"
     ]
    }
   ],
   "source": [
    "# final evaluation\n",
    "print(f\"The word closest to 'stroopwafel' is '{closest_word}' with a cosine similarity of {highest_similarity}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba890329-1411-4922-bd27-fe0490dd1208",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459793e-838d-44fe-8c66-faf8fbe2ca5a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282072da-4993-4490-a263-41fe9799be47",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![assets/imgs/langchain_vectorstores_rag.png](assets/imgs/langchain_vectorstores_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=OLLAMA_EMBEDDING_MODEL,\n",
    "    documents=splits,\n",
    "    embedding=OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ff9c403-d2f3-4b93-89c1-b026192b36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Chroma.delete_collection(vectorstore)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        collection_name=OLLAMA_EMBEDDING_MODEL,\n",
    "        documents=splits,\n",
    "        embedding=OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    "    )\n",
    "    \n",
    "except:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        collection_name=OLLAMA_EMBEDDING_MODEL,\n",
    "        documents=splits,\n",
    "        embedding=OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9290cef9-4d6a-451a-8c59-67432bb24760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['20e581c0-aa84-4c5c-9366-bc9e91adaecc'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'page': 0, 'source': './backend/tmc_tel_lab.pdf'}],\n",
       " 'documents': [\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportunities for growth, and\\nbreakthroughs are celebrated.\\nWhether it's pioneering technology or disruptive healthcare solutions, the Entrepreneurial\\nLab provides the space, resources, and collaborative spirit to turn your vision into a\\nsuccessful venture.\\nHow it works\\nSounds promising, doesn't it? But how exactly does the Entrepreneurial Lab operate?\\n> Our employeneurs work on their own innovation if they desire, but they do this\\nalongside their client project.\"],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.get(limit = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d736b8-d6e0-4da0-afaa-51b81046dfd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bfba0fe9-7bd2-4ec5-90a4-f22ea3ac8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_doc = vectorstore.similarity_search_with_relevance_scores(question, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7dad5c70-8a71-4ec7-b733-0581219a706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b2b33c22-713b-447f-9d2d-77497554e113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the TMC Entrepreneurial Lab?'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40e861be-7551-446c-8a7c-ca9bf0aec1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='alongside their client project.\\n> TMC supports with a physical lab and possibly even with financial resources.\\n> The ownership of the innovation remains of our employeneurs; TMC has no interest in\\nit.\\nBUILDING THE FUTURE IN\\nThe Entrepreneurial Lab\\nThis is the place where you can let your technical dreams come true. If you\\nhave a groundbreaking idea for a new innovation, you can initiate your\\nproject and take it from concept to reality with the support of a vibrant\\ncommunity of like-minded innovators.\\x00 \\x00', metadata={'page': 0, 'source': './backend/tmc_tel_lab.pdf'}),\n",
       "  -111.32334465887976),\n",
       " (Document(page_content=\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportunities for growth, and\\nbreakthroughs are celebrated.\\nWhether it's pioneering technology or disruptive healthcare solutions, the Entrepreneurial\\nLab provides the space, resources, and collaborative spirit to turn your vision into a\\nsuccessful venture.\\nHow it works\\nSounds promising, doesn't it? But how exactly does the Entrepreneurial Lab operate?\\n> Our employeneurs work on their own innovation if they desire, but they do this\\nalongside their client project.\", metadata={'page': 0, 'source': './backend/tmc_tel_lab.pdf'}),\n",
       "  -139.5120848479825),\n",
       " (Document(page_content='the Entrepreneurial Lab might be just the place for you. We offer an environment where\\nyou can directly contribute by working on one of the projects, or conducting a research for\\none of them. Our experienced professionals are keen to guide you along your way as you\\nhelp shape a beter tomorrow.\\n\\x00', metadata={'page': 1, 'source': './backend/tmc_tel_lab.pdf'}),\n",
       "  -153.81020562087747),\n",
       " (Document(page_content='Careers\\nGraduate programs\\nVIE Program\\nCorporate vacancies\\nAll vacancies\\nService areas\\nTechnology & Engineering\\nDigital & IT\\nEnergy & Renewables\\nLife Sciences & Pharma\\nDiscover TMC\\nAbout us\\nUpdates\\nFAQ\\nContact\\n© 2024 TMC |Terms and conditions |Privacy statement |Cookie Statement |Settings', metadata={'page': 3, 'source': './backend/tmc_tel_lab.pdf'}),\n",
       "  -161.7638082100738),\n",
       " (Document(page_content=\"> The first spin-offs have already been rolled out, that is what we call true\\nemployeneurship!\\nCollaboration with our specialists\\nNext to our own projects, we offer support to external parties seeking solutions for\\nspecific project challenges. Whether it’s a technical issue or a strategic challenge, we\\noffer a platform where we will dive into your project. By engaging with us, you gain\\naccess to a pool of top talent, our Employeneurs, who eagerly contribute their expertise\\nand creativity to address your needs. Let's unite our efforts and realize your project's\\ngoals by accessing the combined expertise of our Entrepreneurial Lab's diverse talents\\nand specialized knowledge.\\nInternships\\nIf you are an entrepreneurial student, excited to make an impact in a real-world project,\\nthe Entrepreneurial Lab might be just the place for you. We offer an environment where\", metadata={'page': 1, 'source': './backend/tmc_tel_lab.pdf'}),\n",
       "  -166.8511445711056)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf9e2e-329e-4c35-8f05-2dceea717daa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combine Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d17745b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e4461264-5cac-479a-917c-9bf589826da4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOllama(model=OLLAMA_LARGE_LANGUAGE_MODEL, base_url=OLLAMA_SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5985e-4131-446f-b4e5-baa57ec1929b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Invoking the RAG chain\n",
    "response = chain.invoke({\"context\":returned_doc[0], \"question\":question})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c9497",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the `chain.invoke()` example above we used directly the result output of a similarity search of the vector database. Langchain has a better approach for this via retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9369392f-62d8-4160-9f24-990cd4d7bfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punch\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\"\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sound\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sounding\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sounding like\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sounding like \"\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sounding like \"sky\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sounding like \"sky-\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sounding like \"sky-peer\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sounding like \"sky-peer group\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                   (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  \n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  ,\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times,\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they'\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're a\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're a bit\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're a bit of\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're a bit of a\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're a bit of a dri\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're a bit of a drizz\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're a bit of a drizzle\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Sure, here's a short weather-related joke for you:                                                  , and other times, they're a bit of a drizzle!\n",
      "\n",
      "Why don't clouds ever get lonely? Because they always have plenty of friends in the sky-peer circle! (The punchline plays on \"support group\" sounding like \"sky-peer group.\")\n",
      "\n",
      "Remember, jokes about the weather can be as variable as the weather itself – sometimes they're a hit, and other times, they're a bit of a drizzle!"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def print_stream():\n",
    "    full_message = \"\"\n",
    "    async for chunk in llm.astream(\"Tell me a short joke about the weather.\"):\n",
    "        full_message += chunk.content\n",
    "        print(\"\\r\" + \" \" * 100, end='')  # Clear the line\n",
    "        print(\"\\r\" + full_message, end='')\n",
    "\n",
    "# To run the async function\n",
    "await print_stream()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a51dc-ea11-46b2-b8dc-272bfca63221",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a2ff9-2343-4047-bb07-0a3cd4de73bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# here we create a retriever from the vectorstore which can perform similarity search and returns one document\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d07a22-8234-40fa-973a-b5a84ec14023",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# with this retreiver the context (relevant split) is directry passed to the question addressing the LLM.\n",
    "response = chain.invoke({\"context\":retriever,\"question\":question})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa098889",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748826de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HandsOn --- Answer not in Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087aad15",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> What happens if the answer is not in the splits of any retreived document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60696b2-067e-481d-a147-37032051e5f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "response = chain.invoke({\"context\":retriever,\"question\":\"What is a large language model?\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03957484",
   "metadata": {},
   "source": [
    "#### Better Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65770e2d-3d5e-4371-abc9-0aeca9646885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f53e5840-0a0f-4428-a4a4-6922800aff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e16118-a165-4808-adc7-d89d9d195706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3eaf19-5335-455f-a5fe-e54d6712f23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764f7f1-4cfb-4516-a7e6-f25a80e370fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b9dd43a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" The TMC Entrepreneurial Lab is likely an initiative or program designed to support and foster entrepreneurship, possibly leveraging technology like Chroma for data storage and OllamaEmbeddings for natural language processing within a vector store context. The provided context seems to suggest that it might utilize advanced search capabilities with 'k': 1 for relevant information, indicating a focus on retrieving top-ranked results or data points. Without additional context, the exact nature of the lab and its specific activities remain uncertain.\", response_metadata={'model': 'wizardlm2:7b', 'created_at': '2024-06-17T15:34:32.13742427Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 25822861294, 'load_duration': 8436144, 'prompt_eval_count': 70, 'prompt_eval_duration': 5840493000, 'eval_count': 106, 'eval_duration': 19831949000}, id='run-972d617c-8f92-4671-bbca-f16cc559718e-0')"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "e4c0a8e0-cae9-4e6b-aca5-1851f2e737e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./backend/tmc_tel_lab.pdf'"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" A large language model like GPT-3 is an advanced AI system designed to understand and generate human-like text based on the input it receives. It is trained on vast amounts of data to perform a wide variety of natural language processing tasks, and it can assist with both technical issues and strategic challenges by providing insights or generating content relevant to those areas. The model itself, once trained, does not own the innovations it helps create; rather, the ownership typically remains with the creators or the entity that deployed the model. Large language models are used by various organizations, including TMC's Entrepreneurial Lab, to support employeneers and external parties in solving complex problems.\""
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the final output we might want to know in which document we can find the information of the similarity search.\n",
    "# This is handled by RunnablePassthrough() function which can add a \n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is a large language model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "abc499fa-ccea-436a-8aa8-08a8d4093c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant specialized in question-answering tasks. Use the given context to answer the question concisely. If the answer is not present in the context, clearly state that you don't know the answer and do not provide any further answer.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"You are an assistant specialized in question-answering tasks. Use the given context to answer the question concisely. If the answer is not present in the context, clearly state that you don't know the answer and do not provide any further answer.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69cbc517-4587-4e9f-bec2-25c68692d348",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mretriever\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()}\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m rag_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is a large language model?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is a large language model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe29a1-5527-419e-9f12-8a3061d12885",
   "metadata": {},
   "source": [
    "[RAG chains](https://python.langchain.com/docs/expression_language/get_started#rag-search-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9fb95-db25-4a0d-ad21-2462e84c6206",
   "metadata": {},
   "source": [
    "## HandsOn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec39d-8bff-4c49-8582-268921e7854d",
   "metadata": {},
   "source": [
    "1. Rerun the overall app using a question which relates to the [website](https://www.themembercompany.com/nl/employeneurship). You first will have to load the website via WebBaseLoader.\n",
    "2. Directly ask the llm something ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35599efd-fc0c-4daf-955a-931e8cfd3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOllama(model=OLLAMA_LARGE_LANGUAGE_MODEL, base_url=OLLAMA_SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad44b74d-48af-4bfe-a0f9-ef6eeae0313c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x7ffa3b3e5a20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = llm.invoke(\"Tell me a joke about the weather.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a33ae01-b7d4-4e9e-9240-7c00d9f53bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s a light-hearted weather joke for you:\\n\\nWhy don\\'t clouds ever make good witnesses?\\n\\nBecause they can\\'t be relied upon; they\\'re full of hot air and often have a \"foggy\" memory!\\n\\nAnd here\\'s another one:\\n\\nDid you hear about the argument between the weatherman and the physicist?\\n\\nThe weatherman said, \"Tomorrow will be a perfect day.\" The physicist replied, \"That\\'s impossible. Every eventuality is associated with an equal and opposite probability!\" To which the weatherman responded, \"Well, I guess you could say it has a 50/50 chance of being a perfect day!\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f197a-7dc4-42c5-a301-82634932b53e",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff57604-cb72-43c9-ba68-3131a2f0b282",
   "metadata": {},
   "source": [
    "Implementation of Simple Retrieval-Augmented Generation (RAG) from Scratch\n",
    "- **Ingestion Phase**:\n",
    "    - **Load**: Loading documents into the system.\n",
    "    - **Split**: Splitting documents into manageable chunks.\n",
    "    - **Embed**: Embedding document chunks into vector representations.\n",
    "    - **Store**: Storing the embedded documents in a vector store.\n",
    "- **Similarity Search**: Searching for relevant documents using embedded query.\n",
    "- **Combine Context**: Combining the retrieved document context with the query.\n",
    "- **Response Generation**: Generating the final response using a Large Language Model (LLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb00b8-cb17-4ae5-aef4-e0be6e89c070",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0a9c4-076a-4da9-bde9-f51cb30ee87a",
   "metadata": {},
   "source": [
    "There is plenty more to discover at [langchain's](https://github.com/langchain-ai) and many other websites! Especially check out: [YouTube](https://www.youtube.com/watch?v=sVcwVQRHIc8&ab_channel=freeCodeCamp.org) and [Github](https://github.com/langchain-ai/rag-from-scratch/tree/main). Here an overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33306e55-0ce9-4db7-ba48-5355311b5cd9",
   "metadata": {},
   "source": [
    "![assets/imgs/langchain_rag_overview.png](assets/imgs/langchain_rag_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73c187-64ac-4b6e-a194-049282b5ca1b",
   "metadata": {},
   "source": [
    "## TMChampionship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e3488-ccc1-400f-953c-fbc0fe6a9568",
   "metadata": {},
   "source": [
    "* TEL organises/-ed a project journey towards a shark tank like investor pitch in November\n",
    "* Milan and I - started TMChampionship project Prometheon.ai to build a sustainable manufacturing knowledge expert\n",
    "* TEL has many wonderful opportunities for you to try new things, learn, connect and especially grow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19445455-22ab-451a-a28c-e52c52112235",
   "metadata": {},
   "source": [
    "## Thank You"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9dc9a-78d4-4088-87aa-17a972db96bf",
   "metadata": {},
   "source": [
    "- **Technical Support**: Milan and Raul\n",
    "- **Organisational Support**: Marlies, Wendy, and Varsha\n",
    "- **Motivational Support**: TMChampionship/ TEL/ Pepijn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed862a6c-8654-4e9c-beaa-a600d85b9496",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e65d82-c245-44d9-bae6-c9fadc326856",
   "metadata": {},
   "source": [
    "![assets/imgs/ai_workshop_tmc__feedback.png](assets/imgs/ai_workshop_tmc__feedback.png)\n",
    "\n",
    "https://forms.office.com/e/CwRvint3LY?origin=lprLink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fdf7bb-0415-4bec-b3b9-412f09e1eba6",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4398734-192f-4ee4-bf37-47bbc7aec96f",
   "metadata": {},
   "source": [
    "- **credits**: this notebook heavily borrows from langchain's [rag_from_scratch_1_to_4.ipynb](\"https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8a69a-5f46-4672-97ac-24ae362f04b5",
   "metadata": {},
   "source": [
    "* **GPT**: Generative Pre-trained Transformer, a type of language model developed by OpenAI that generates human-like text using transformer architecture.\n",
    "* **LLM**: Large Language Model, a machine learning model trained on vast amounts of text data to understand and generate human language.\n",
    "* **Transformer**: Deep learning model using attention mechanism for context understanding and parallel processing, introduced in the \"Attention is All You Need\" paper.\n",
    "* **Embedding Models**: Convert text to vector representations (e.g., BERT).\n",
    "* **Generation Models**: Generate text from prompts (e.g., GPT-3).\n",
    "* **Softmax Function**: Converts values to probabilities, used in classification models.\n",
    "* **Fine-Tune vs. Retrieval \"Augmented Generation**\n",
    "    * **Fine-Tuning an LLM**: Adapts model to specific tasks using labeled data.\n",
    "    * **RAG (Retrieval-Augmented Generation)**: Combines retrieval with generation for context-specific responses.\n",
    "* **Micro Timeline**\n",
    "    * **2017**: \"Attention is All You Need\" paper.\n",
    "    * **2018**: BERT, GPT-2\n",
    "    * **2020**: GPT-3.\n",
    "* **Quantization**: Reduces precision of model parameters.\n",
    "    * **Benefits**: Smaller size, faster inference, lower power consumption.\n",
    "    * **Types**: Static, Dynamic, Quantization-Aware Training.\n",
    "    * **Challenges**: Accuracy loss, hardware support needed.\n",
    "* **Not all LLMs are GPTs**: Other models include BERT, T5, XLNet, RoBERTa.\n",
    "* **Not all LLMs use transformers**: Other architectures include RNNs, CNNs, MoE, Memory-Augmented Networks."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
