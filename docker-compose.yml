services:
  jupyter:
    container_name: jupyter
    # Build configuration
    build:
      # Set the build context to the current directory
      context: ./jupyter
      # Use the Dockerfile in the current directory
      dockerfile: Dockerfile
    # Use the base-notebook:ubuntu-22.04 image of jupyter/base-notebook
    # image: quay.io/jupyter/base-notebook:ubuntu-22.04
    # Set environment variables
    extra_hosts:
      - host.docker.internal:host-gateway
    environment:
      # Set the Jupyter token using the JUPYTER_PASSWORD environment variable
      # - OLLAMA_BASE_URLS=http://host.docker.internal:11434
      - OLLAMA_BASE_URL=http://ollama:11434
      - JUPYTER_TOKEN=${JUPYTER_PASSWORD}
      - NB_USER="jovyan"
      - CHOWN_HOME=yes
      - GRANT_SUDO=yes
    # Mount the local file ai_workshop_tmc.ipynb to the container
    volumes:
      - ./:/home/jovyan
      # Command to start the Jupyter notebook
    command: start-notebook.sh
    # Ports configuration to map host ports to container ports
    ports:
      # Map port 8888 on the host (left number, your laptop) to port 8888 on the container for Jupyter
      - "8888:8888"
    # Connect the container to the ai_workshop_tmc__network
    networks:
      - ai_workshop_tmc__network
    # Restart the container unless it is explicitly stopped
    restart: unless-stopped
    # Healthcheck to monitor the health of the container
    healthcheck:
      # Healthcheck to monitor the health of the container
      # Test command to check if the service is healthy
      # It runs "curl -f http://localhost:8888" and exits with status 1 if the command fails
      test: ["CMD", "curl", "-f", "http://localhost:8888"]
      # Time between running the check (1 minute 30 seconds)
      interval: 1m30s
      # Maximum time to allow one check to run (10 seconds)
      timeout: 10s
      # Number of consecutive failures needed to consider the container as unhealthy
      retries: 3
    # Set the working directory
    working_dir: /home/jovyan
    depends_on:
      # This service depends on the chroma service
      # - chroma
      # This service depends on the ollama service
      - ollama

  backend:
    container_name: backend
    build:
      context: ./backend
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - ./backend:/app
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    ports:
      - "8001:8001"
    # command: /bin/bash
    command: ["poetry", "run", "uvicorn", "server:app", "--host", "0.0.0.0", "--port", "8001", "--reload"]
    # Connect the container to the ai_workshop_tmc__network
    networks:
      - ai_workshop_tmc__network
    # Restart the container unless it is explicitly stopped
    restart: unless-stopped
    healthcheck:
      # Test command to check if the service is healthy
      # It runs "curl -f http://localhost:8000" and exits with status 1 if the command fails
      test: ["CMD", "curl", "-f", "http://localhost:8001"]
      # Time between running the check (1 minute 30 seconds)
      interval: 1m30s
      # Maximum time to allow one check to run (10 seconds)
      timeout: 10s
      # Number of consecutive failures needed to consider the container as unhealthy
      retries: 3
    # Set the working directory
    working_dir: /app
    depends_on:
      # This service depends on the chroma service
      # - chroma
      # This service depends on the ollama service
      - ollama
    
  frontend:
    container_name: frontend
    build:
      context: ./frontend
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - ./frontend:/app
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    ports:
      - "8501:8501"
    command: ["poetry", "run", "streamlit", "run", "/app/app.py", "--server.port=8501", "--server.address=0.0.0.0", "--server.fileWatcherType=auto"]
    # Connect the container to the ai_workshop_tmc__network
    networks:
      - ai_workshop_tmc__network
    # Restart the container unless it is explicitly stopped
    restart: unless-stopped
    healthcheck:
      # Test command to check if the service is healthy
      # It runs "curl -f http://localhost:8501" and exits with status 1 if the command fails
      test: ["CMD", "curl", "-f", "http://localhost:8501"]
      # Time between running the check (1 minute 30 seconds)
      interval: 1m30s
      # Maximum time to allow one check to run (10 seconds)
      timeout: 10s
      # Number of consecutive failures needed to consider the container as unhealthy
      retries: 3
    # Set the working directory
    working_dir: /app
    depends_on:
      # This service depends on the chroma service
      # - chroma
      # This service depends on the ollama service
      - ollama
      - backend

  # unstructured:
  #   container_name: unstructured
  #   # Use the unstructured-api:0.0.69 image of unstructured-api from Quay.io
  #   image: quay.io/unstructured-io/unstructured-api:0.0.69
  #   # # Deploy the container with resource reservations and limitations
  #   # deploy:
  #   #   resources:
  #   #     limits:
  #   #       cpus: "0.50"
  #   #       memory: "512M"
  #   #     reservations:
  #   #       cpus: "0.25"
  #   #       memory: "256M"
  #   #       devices:
  #   #         # Reserve GPU resources for the container
  #   #         - capabilities: [gpu]
  #   # Set the command to run the API on port 8000 and bind it to all network interfaces
  #   command: ["--port", "8000", "--host", "0.0.0.0"]
  #   # Mount the local directory ./ingestion_data to /home/notebook-user/ingestion_data in the container
  #   volumes:
  #     - ./ingestion_data:/home/notebook-user/ingestion_data
  #   # Map port 7001 on the host to port 8000 on the container
  #   ports:
  #     - "8003:8000"
  #   # Connect this service to the ai_workshop_tmc__network network
  #   networks:
  #     - ai_workshop_tmc__network
  #   # Restart the container unless it is explicitly stopped
  #   restart: unless-stopped
  #   # Define a health check to monitor the container's health status
  #   healthcheck:
  #     # Command to check the health endpoint
  #     test: ["CMD-SHELL", "curl --fail http://localhost:8000 || exit 1"]
  #     # Time between running the health check command
  #     interval: 1m30s
  #     # Maximum time to wait for the health check command to complete
  #     timeout: 10s
  #     # Number of consecutive failures needed to mark the container as unhealthy
  #     retries: 3
  #     # Time to wait after starting before beginning health checks
  #     start_period: 40s

  # chroma:
  #   container_name: chroma
  #   # Use the chroma:0.5.1.dev172 image of Chroma from GitHub Container Registry
  #   image: ghcr.io/chroma-core/chroma:0.5.1.dev172
  #   # Map port 8000 on the host to port 8000 on the container
  #   ports:
  #     - "8004:8000"
  #   # Mount the local directory ./vector_database/index to /chroma/.chroma/index in the container
  #   volumes:
  #     - ./vector_database/index:/chroma/.chroma/index
  #   # Connect this service to the ai_workshop_tmc__network network
  #   networks:
  #     - ai_workshop_tmc__network
  #   # Restart the container unless it is explicitly stopped
  #   restart: unless-stopped
  #   # Deploy the container with appropriate resource limitations
  #   # deploy:
  #   #   resources:
  #   #     limits:
  #   #       cpus: "0.50"
  #   #       memory: "512M"
  #   #     reservations:
  #   #       cpus: "0.25"
  #   #       memory: "256M"
  #   # Healthcheck to monitor the health of the container
  #   healthcheck:
  #     # Test command to check if the service is healthy
  #     # It runs "curl -f http://localhost:8000/" and exits with status 1 if the command fails
  #     test: ["CMD-SHELL", "curl -f http://localhost:8000/ || exit 1"]
  #     # Time between running the check (1 minute 30 seconds)
  #     interval: 1m30s
  #     # Maximum time to allow one check to run (10 seconds)
  #     timeout: 10s
  #     # Number of consecutive failures needed to consider the container as unhealthy
  #     retries: 3
  #     # Start period to wait before starting health checks (40 seconds)
  #     start_period: 40s

  # The heart of the application: the large language model server ollama.
  ollama:
    container_name: ollama
    # Use the specified version of the ollama image
    image: ollama/ollama:0.1.44
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "0.50"
    #       memory: "512M"
    #     reservations:
    #       cpus: "0.25"
    #       memory: "256M"
    #       # Uncomment the following lines if you need to reserve GPU resources
    #       # devices:
    #       # - capabilities: [gpu]
    tty: true
    # environment:
    #   - OLLAMA_KEEP_ALIVE=24h
    volumes:
      # # Mount Docker socket to communicate with Docker daemon
      # - /var/run/docker.sock:/var/run/docker.sock
      # Mount local llm directory to container's .ollama directory
      - ./llm:/root/.ollama
    ports:
      # Map port 11434 of the host to port 11434 of the container
      - "11434:11434"
    # Use /bin/sh as the entrypoint to run shell commands
    entrypoint: ["/bin/sh", "-c", "/bin/ollama serve"] # export OLLAMA_HOST=0.0.0.0:11434 && export OLLAMA_ORIGINS=http://ai_workshop_tmc-langserve-1:*,http://ai_workshop_tmc-streamlit-1:*,http://ai_workshop_tmc-jupyter-1:* && 
    # Start the ollama service and then run the specified command
    command: ["ollama run ${OLLAMA_LARGE_LANGUAGE_MODEL}"]
    # network_mode: host
    networks:
      # Connect to the specified Docker network
      - ai_workshop_tmc__network
    # Restart the container unless it is explicitly stopped
    restart: unless-stopped
    healthcheck:
      # Check if the service is responding on port 11434
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      # Time between running the check (1 minute 30 seconds)
      interval: 1m30s
      # Time to wait for the check to complete (10 seconds)
      timeout: 10s
      # Number of consecutive failures needed to consider the container as unhealthy
      retries: 3
      # Start period to allow the container to initialize before starting health checks (40 seconds)
      start_period: 40s 
  
  ## LLM
  # llama3:instruct
  # docker exec ollama ollama pull llama3:instruct
  # docker exec ollama ollama run llama3:instruct

  ## LLM
  # tinyllama:chat
  # docker exec ollama ollama pull tinyllama:chat
  # docker exec ollama ollama run tinyllama:chat

  ## LLM
  # wizardlm2:7b
  # docker exec ollama ollama pull wizardlm2:7b
  # docker exec ollama ollama run wizardlm2:7b

  ## Embedding
  # mxbai-embed-large
  # docker exec ollama ollama pull mxbai-embed-large

  ## Embedding
  # all-minilm
  # docker exec ollama ollama pull all-minilm

networks:
  # Define a custom network named 'ai_workshop_tmc__network'
  ai_workshop_tmc__network:
    # Use the 'bridge' driver for the network
    driver: bridge
