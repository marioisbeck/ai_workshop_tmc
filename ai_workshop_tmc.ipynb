{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Part II: Building the AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a4c19-69f0-45ff-93fd-18bc96358d94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e9bdb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) Vaswani et al. (2017, Google Brain/ Research)\n",
    "* 5 days to 1 million users (OpenAI)\n",
    "* 1.8 billion monthly visits in March 2023 (OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cd24b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Agricultural Revolution**: Around 10,000 BCE, shift to settled farming.\n",
    "- **Industrial Revolution**: Late 18th century, rise of industrialization.\n",
    "- **Digital (Computer) Revolution**: Mid-20th century, advent of computers.\n",
    "- **AI Revolution**: Early 21st century, integration of artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f52aeb-e03e-445e-898f-50995dd79148",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae92ca-3e92-4b2f-8b78-10130d41749e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Introduction to LLMs\n",
    "* Starting Docker Containers\n",
    "* Build AI Assistant (Walkthrough & HandsOn)\n",
    "    * Ingestion (Load, Split, Embed, Store)\n",
    "    * Similarity Search\n",
    "    * Combine Context\n",
    "    * Response Generation\n",
    "* Langserve and Streamlit App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192fd6c0-9607-4a12-bccd-f8183153597b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We will build a simple retrieval augmented generation (RAG) pipeline and complete HandsOn tasks.\n",
    "* The notebook is based on [langchains rag intro](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb).\n",
    "* We build towards a broader understanding of the RAG langscape langchain's [rag from scratch](https://github.com/langchain-ai/rag-from-scratch/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2cb58-bb54-4629-a0c9-4e1afe38140b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812f0d9-1fc8-43ba-b72a-5989710ff4f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Complete Installation and Understand Functioning of Essential Tools  \n",
    "2. **Understand the Basics of Large Language Models (LLMs)**\n",
    "3. **Understand on a Programmatic Level how AI Assistants are Built**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836d5ad-e281-4977-8ca7-285fbe1ce20d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting The Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c2c35-7f61-40bd-9b1b-01f78ba74c4a",
   "metadata": {},
   "source": [
    "### Clone GitHub Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064f738-58cc-4365-9800-11a3803f54af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "# Clone the repository from the given URL\n",
    "git clone https://github.com/marioisbeck/ai_workshop_tmc.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "cd ai_workshop_tmc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa577f8-1489-495e-8fb6-71391f6dba1e",
   "metadata": {},
   "source": [
    "### Run Docker Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5b050-5b1e-4e50-be59-9d55686fd1da",
   "metadata": {},
   "source": [
    "#### Mac/ Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7f1fc-ddd7-49d4-82ab-58a2cfef17bb",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Copy the example environment file to a new .env file\n",
    "cp .env.example .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c8489-2a55-4302-8098-60735cc0d8cd",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Build and start the Docker containers in detached mode\n",
    "docker-compose up -d --build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7689cf7-bc9b-4e8a-9688-66169acf8022",
   "metadata": {},
   "source": [
    "#### Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421e09a-64f3-4769-bba6-6a3a9c6770b0",
   "metadata": {},
   "source": [
    "```powershell\n",
    "# Copy the example environment file to a new .env file\n",
    "Copy-Item -Path \".env.example\" -Destination \".env\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3642851-7f8d-4afb-bdad-1d2b41ed09e4",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Build and start the Docker containers in detached mode\n",
    "docker-compose up -d --build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c13a4-b319-48c0-bec9-0d71cee4528a",
   "metadata": {},
   "source": [
    "### Download LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32f79c-7772-457c-9800-5afabae3ef15",
   "metadata": {},
   "source": [
    "#### Mac/ Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a60181-8388-4eac-8298-dd9dccd85c0e",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Pull the mxbai-embed-large model in the ollama container\n",
    "docker exec ollama ollama pull mxbai-embed-large && \\\n",
    "# Pull the all-minilm model in the ollama container\n",
    "docker exec ollama ollama pull all-minilm && \\\n",
    "# Pull the wizardlm2:7b model in the ollama container\n",
    "docker exec ollama ollama pull wizardlm2:7b && \\\n",
    "# Run the wizardlm2:7b model in the ollama container\n",
    "docker exec ollama ollama run wizardlm2:7b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcd5f7-d407-47df-bae1-520aa0a8f4f9",
   "metadata": {},
   "source": [
    "#### Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce7783a-e6d4-4954-bbbd-e2418c41226a",
   "metadata": {},
   "source": [
    "```powershell\n",
    "# Pull the mxbai-embed-large model in the ollama container\n",
    "docker exec ollama ollama pull mxbai-embed-large; `\n",
    "# Pull the all-minilm model in the ollama container\n",
    "docker exec ollama ollama pull all-minilm; `\n",
    "# Pull the wizardlm2:7b model in the ollama container\n",
    "docker exec ollama ollama pull wizardlm2:7b; `\n",
    "# Run the wizardlm2:7b model in the ollama container\n",
    "docker exec ollama ollama run wizardlm2:7b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42eae3-e625-4411-9d9c-d85a73604b3a",
   "metadata": {},
   "source": [
    "### Test Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76256fed-add6-44da-9a3f-baeac368b760",
   "metadata": {},
   "source": [
    "Open `Docker Desktop` and go to `Containers`. You should see something like this (all container icons should be green):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340abf1-e8d4-4a2c-a5a9-3f712411445b",
   "metadata": {},
   "source": [
    "![assets/imgs/docker-compose_success.png](assets/imgs/docker-compose_success.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3cb412-5330-46ad-9df5-7a7fe741ad95",
   "metadata": {},
   "source": [
    "* **Application** Go to [http://localhost:8501](http://localhost:8501) and see if the frontend is there, upload a pdf (<5MB) and ask a question about it. If everything goes well, it should answer ðŸ˜Š.\n",
    "* **Jupyter** Go to [http://localhost:8888](http://localhost:8888) where you should see the option to run the ai_workshop_tmx.ipynb notebook. Within this notebook you can follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c501b-f64b-42ff-ad8f-6deb3605fb71",
   "metadata": {},
   "source": [
    "### Remove Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d228dc-511f-4c1b-96c2-75c85d026899",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Stop and remove all containers defined in the docker-compose.yml file\n",
    "docker-compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4090af-9589-4cc0-8f4c-78458b6a62b7",
   "metadata": {},
   "source": [
    "## Client Journey "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40332dfc-7788-4aee-a1ad-d8bbf0e525df",
   "metadata": {},
   "source": [
    "* programming interest, proactive approach\n",
    "* presented working chatbot prototype to client\n",
    "* client invested in dedicated GPU server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6dec99-07aa-41e5-96a3-f1bb1db7a8a4",
   "metadata": {},
   "source": [
    "## TMChampionship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96eb17-1a0e-4535-8f1a-1ea3750f8816",
   "metadata": {},
   "source": [
    "* TEL organises/-ed a project journey towards a shark tank like investor pitch in November\n",
    "* Milan and I - started TMChampionship project Prometheon.ai to build a sustainable manufacturing knowledge expert\n",
    "* TEL has many wonderful opportunities for you to try new things, learn, connect and especially grow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b6bc2-17f8-4ab7-a9fb-c65ab164ac18",
   "metadata": {},
   "source": [
    "## Prometheon.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b6349-f69b-47e4-b8c2-e2ce673e8854",
   "metadata": {},
   "source": [
    "[www.prometheon.ai](https://www.prometheon.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617e1d6-5da8-459d-bc3d-31d9dc741862",
   "metadata": {},
   "source": [
    "**Theon**\n",
    "> Theon helps manufacturing companies manage their data securely on-site and integrates with existing systems to improve decision-making and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb2275-618a-41c5-840d-91917437d47b",
   "metadata": {},
   "source": [
    "**Rachel**\n",
    "> Rachel offers expert guidance on sustainable manufacturing through an AI chat interface, integrating supplier details, company data, and resources to promote eco-friendly practices while ensuring efficiency and profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab14572-84d8-4e92-a999-2f5cabfe9d32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fe82f-a383-4f2a-b68d-1ad408d866bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Slides are shamelessly taken from 3Blue1Brown's [But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning](https://www.youtube.com/watch?v=wjZofJX0v4M&ab_channel=3Blue1Brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b109fa-ec86-42bc-a09a-ac5b79148a5d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![/overview.jpeg](assets/imgs/overview.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ffd28-4838-48d7-bc8c-b5e671b90d1d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![/tokens.jpeg](assets/imgs/tokens.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df524967-2689-49da-b785-545a0abc524a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![/giving_meaning.jpeg](assets/imgs/giving_meaning.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780faa85-325c-45ce-a1a6-2185b16a61b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7374b2-ce57-4511-be6a-e5130984c9e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![assets/imgs/simple_rag.png](assets/imgs/simple_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963535df",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- **Ingestion**: Load and preprocess documents for further processing.\n",
    "    - **Load**: Upload documents to the backend.\n",
    "    - **Split**: Split documents into manageable chunks using characters, sections, semantic meaning, and delimiters.\n",
    "    - **Embed**: Convert document chunks (and query) into vector embeddings for representation.\n",
    "    - **Store**: Store the embeddings in a vector database (Vectorstore) for efficient retrieval.\n",
    "- **Similarity Search**: Use the query embedding to search and retrieve the most relevant document chunks from the Vectorstore.\n",
    "- **Combine Context**: Combine retrieved document chunks with the query to provide context for the generation model.\n",
    "- **Response Generation**: Use a language model to generate a response based on the query and retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434f741",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832b105-edf7-4160-9e76-ac5d351337fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Keep it Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871a327-6b29-4a71-ba29-5cb4392b0fff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following is only to suppress output which we do not care about in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219cb0fc-1209-4ae5-ba7e-2b3b31119aa2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Setting USER_AGENT variable for jupyter notebook\n",
    "os.environ['USER_AGENT'] = 'jovyan'\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Disable info messages\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb312d-8a07-4df3-8462-72ac526715f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import bs4  # Library for web scraping and parsing HTML/XML\n",
    "from langchain import hub  # Access langchain hub for pre-built tools and models\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Tool to split text recursively by characters\n",
    "from langchain_core.output_parsers import StrOutputParser  # Parses output into strings\n",
    "from langchain_core.runnables import RunnablePassthrough  # Pass-through runnable for data processing\n",
    "from langchain_community.chat_models import ChatOllama  # Chat model from Langchain Community\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings  # Ollama embeddings for text representation\n",
    "from langchain_community.document_loaders import WebBaseLoader  # Load documents from the web\n",
    "from langchain_community.vectorstores import Chroma  # Chroma vector store for efficient retrieval\n",
    "from langchain_community.document_loaders import PyPDFLoader # reading in pdfs\n",
    "from langchain.prompts import ChatPromptTemplate # class for promts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cb8c7-eb50-4a78-a199-e35ee99f70c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a5073d-13dc-4efd-a678-844268160afa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "OLLAMA_LARGE_LANGUAGE_MODEL = \"wizardlm2:7b\"  # Specifies the large language model version\n",
    "OLLAMA_SERVER = \"http://ollama:11434\"  # URL for the Ollama server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0a964e-d7d5-46f3-bc88-645023b6a615",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is the TMC Entrepreneurial Lab?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cae3a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492978da-2e2b-4e99-a399-754e17092e29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4600aa-4036-4bee-898a-0029df3ae55e",
   "metadata": {},
   "source": [
    "![assets/imgs/simple_rag.png](assets/imgs/simple_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952dd1ab-246a-4962-a34a-94ac01ae08eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a72179-477a-4769-b161-563b01ffc6a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071aa7c5-f6e6-45a0-babb-1bdb364b70b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# pdf document loader\n",
    "loader = PyPDFLoader(\n",
    "    \"./backend/tmc_tel_lab.pdf\"\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c62ac47-eb7d-4d8e-b620-2d9a61de25e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43c96b9-4866-43de-8abe-6cacc65c4697",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, dri\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce950f21-f367-416e-a3ee-2e6e112969f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HandsOn --- Web Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac2e60-8327-4fad-8817-55b090073e56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> How can you use the WebBaseLoader to load the contents of the following website: \"https://www.themembercompany.com/nl/employeneurship\"?\n",
    "\n",
    "> How long is the page_content of the resulting document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5778c31a-6138-4130-8865-31a08e82b9fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# # HandsOn: - Web Loader\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://www.themembercompany.com/nl/employeneurship\",)\n",
    "# )\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f04f1c4-ddf2-4c00-b3dd-8154e3ef994a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# # show character length page content\n",
    "# len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e55bc-acee-4378-9842-a20a504c5e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628c52b-dd3b-46a2-867d-115c61e7824b",
   "metadata": {},
   "source": [
    "![assets/imgs/simple_rag__load.png](assets/imgs/simple_rag__load.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85118b9-cfed-4e89-9c02-75cd1be5ad1f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Splitter](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e668d339-3951-4662-8387-c3d296646906",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200, \n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca169f93-215a-4a99-bb28-b5b41d62aa08",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46ca07fc-0036-4a60-9912-c7568d60e862",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359faa46-81d6-4832-a193-5f2ad5009c47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportuniti\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e70dfee8-75c6-4504-87c2-a46673488b8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportunities for growth, and\\nbreakthroughs are celebrated.\\nWhether it's pioneering technology or disruptive healthcare solutions, the Entrepreneurial\\nLab provides the space, resources, and collaborative spirit to turn your vision into a\\nsuccessful venture.\\nHow it works\\nSounds promising, doesn't it? But how exactly does the Entrepreneurial Lab operate?\\n> Our employeneurs work on their own innovation if they desire, but they do this\\nalongside their client project.\\n> TMC supports with a physical lab and possibly even with financial resources.\\n> The ownership of the innovation remains of our employeneurs; TMC has no interest in\\nit.\\nBUILDING THE FUTURE IN\\nThe Entrepreneurial Lab\\nThis is the place where you can let your technical dreams come true. If you\\nhave a groundbreaking idea for a new innovation, you can initiate your\\nproject and take it from concept to reality with the support of a vibrant\\ncommunity of like-minded innovators.\\x00 \\x00\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c580505e-c75d-4749-8c52-b3171654542c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportunities for growth, and\\nbreakthroughs are celebrated.\\nWhether it's pioneering technology or disruptive healthcare solutions, the Entrepreneurial\\nLab provides the space, resources, and collaborative spirit to turn your vision into a\\nsuccessful venture.\\nHow it works\\nSounds promising, doesn't it? But how exactly does the Entrepreneurial Lab operate?\\n> Our employeneurs work on their own innovation if they desire, but they do this\\nalongside their client project.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238b3a45-8093-4694-8697-6406c5b9c146",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alongside their client project.\\n> TMC supports with a physical lab and possibly even with financial resources.\\n> The ownership of the innovation remains of our employeneurs; TMC has no interest in\\nit.\\nBUILDING THE FUTURE IN\\nThe Entrepreneurial Lab\\nThis is the place where you can let your technical dreams come true. If you\\nhave a groundbreaking idea for a new innovation, you can initiate your\\nproject and take it from concept to reality with the support of a vibrant\\ncommunity of like-minded innovators.\\x00 \\x00'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba267a-bc98-4020-bbdf-38f3e0a3d849",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d826b9-d27e-40e5-a680-7c408a5b2d89",
   "metadata": {},
   "source": [
    "![assets/imgs/simple_rag__embed.png](assets/imgs/simple_rag__embed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Text embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615960ac-fe6c-4382-96d2-7daa65d9d358",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14444a47-40d4-4d30-8990-396318d6ddf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "OLLAMA_EMBEDDING_MODEL = \"mxbai-embed-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d6378c5-2836-4a98-8d3d-e1a4aa4ab8e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    "query_result = embedding.embed_query(question)\n",
    "split_result = embedding.embed_query(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "596cfd58-11c9-45d3-94df-52580ab49daa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f790c8-28ab-404a-8249-09b7b683ffb7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f717689a-9cbd-4259-9ef7-802b09f05440",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6084602475166321,\n",
       " 0.10699772834777832,\n",
       " -0.6222756505012512,\n",
       " -0.09857344627380371]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_result[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe97626-8cf4-4944-bf85-839e38e07745",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) is reccomended (1 indicates identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12668e52-bad0-4b0c-9c12-861e3c5b6c4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2, print_output = False):\n",
    "    \n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    \n",
    "    if print_output:\n",
    "        print(\"Cosine Similarity:\", similarity)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0e464ae-6b39-4fa3-a758-5dc7e1cdc89a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7252169697102998\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(query_result, split_result, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379847f-a71f-4b39-aac8-22afbe0eb092",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HandsOn --- Better Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19f006-cc6f-47a1-a300-16786601cc64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Write code to use the more sophisticated `mxbai-embed-large` instead of the `all-miniml` embedding model with the local Ollama instance. This enables better performance and more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39b29cf5-23ed-4c9c-8bc0-76e64e128cbf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# OLLAMA_EMBEDDING_MODEL = \"all-minilm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db83ad60-8290-4636-b426-fee4c8cde817",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# embedding = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    "# query_result = embedding.embed_query(question)\n",
    "# split_result = embedding.embed_query(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "588c2cf6-4e9c-46c7-89ab-5cfc4f9f13b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# similarity = cosine_similarity(query_result, split_result, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9fdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HandsOn --- Stroopwafel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d587e2c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Similar to the `Japan - Germany` example from the `Introduction to LLMs` we will now calculate the distance between Netherlands and Germany in the vector space. This we can then use to understand what item in Germany corresponds to what the stroopwafel is in the Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abc0e648",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "OLLAMA_EMBEDDING_MODEL = \"mxbai-embed-large\"\n",
    "embedding = OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b8cc653",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming embedding is some pre-trained embedding model with an embed_query method\n",
    "words = [\"Bratwurst\", \"Mercedes\", \"SchwarzwÃ¤lder Kirschtorte\", \"Berliner\", \"Lebkuchen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6e975ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# getting vectors of tokens/ words\n",
    "stroopwafel_embedding = np.array(embedding.embed_query(\"Stroopwafel\"))\n",
    "netherlands_embedding = np.array(embedding.embed_query(\"Netherlands\"))\n",
    "germany_embedding = np.array(embedding.embed_query(\"Germany\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25a9d2d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# calculating the comparison vector\n",
    "comparison_embedding = stroopwafel_embedding - (netherlands_embedding - germany_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0de488df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initiating variables\n",
    "highest_similarity = -1\n",
    "closest_word = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54dd1d9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bratwurst\n",
      "Cosine Similarity: 0.7104238314729429\n",
      "\n",
      "Mercedes\n",
      "Cosine Similarity: 0.5495463777308662\n",
      "\n",
      "SchwarzwÃ¤lder Kirschtorte\n",
      "Cosine Similarity: 0.6412197961550299\n",
      "\n",
      "Berliner\n",
      "Cosine Similarity: 0.7296874018682962\n",
      "\n",
      "Lebkuchen\n",
      "Cosine Similarity: 0.6988265639666382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running the loop\n",
    "for word in words:\n",
    "    \n",
    "    # embedding the query word\n",
    "    word_embedding = np.array(embedding.embed_query(word))\n",
    "\n",
    "    # generating output\n",
    "    print(word)\n",
    "    similarity = cosine_similarity(comparison_embedding, word_embedding, True)\n",
    "    print(\"\")\n",
    "\n",
    "    # capturing highest similarity\n",
    "    if similarity > highest_similarity:\n",
    "        highest_similarity = similarity\n",
    "        closest_word = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe6757bc-19f4-4e6f-ae27-4e1664a7f049",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word closest to 'stroopwafel' is 'Berliner' with a cosine similarity of 0.7296874018682962.\n"
     ]
    }
   ],
   "source": [
    "# final evaluation\n",
    "print(f\"The word closest to 'stroopwafel' is '{closest_word}' with a cosine similarity of {highest_similarity}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba890329-1411-4922-bd27-fe0490dd1208",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20763254-fffa-4b34-a0c1-8024b074b7cf",
   "metadata": {},
   "source": [
    "![assets/imgs/simple_rag__store.png](assets/imgs/simple_rag__store.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459793e-838d-44fe-8c66-faf8fbe2ca5a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=OLLAMA_EMBEDDING_MODEL,\n",
    "    documents=splits,\n",
    "    embedding=OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ff9c403-d2f3-4b93-89c1-b026192b36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Chroma.delete_collection(vectorstore)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        collection_name=OLLAMA_EMBEDDING_MODEL,\n",
    "        documents=splits,\n",
    "        embedding=OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    "    )\n",
    "    \n",
    "except:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        collection_name=OLLAMA_EMBEDDING_MODEL,\n",
    "        documents=splits,\n",
    "        embedding=OllamaEmbeddings(model=OLLAMA_EMBEDDING_MODEL, base_url=OLLAMA_SERVER)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9290cef9-4d6a-451a-8c59-67432bb24760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['6ba9d156-ab8e-4b7b-b81e-a4a02f0b897a'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'page': 3, 'source': './backend/tmc_tel_lab.pdf'}],\n",
       " 'documents': ['Careers\\nGraduate programs\\nVIE Program\\nCorporate vacancies\\nAll vacancies\\nService areas\\nTechnology & Engineering\\nDigital & IT\\nEnergy & Renewables\\nLife Sciences & Pharma\\nDiscover TMC\\nAbout us\\nUpdates\\nFAQ\\nContact\\nÂ© 2024 TMC |Terms and conditions |Privacy statement |Cookie Statement |Settings'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.get(limit = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d736b8-d6e0-4da0-afaa-51b81046dfd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d326a-65ff-4e47-8ccb-96beec31e4c5",
   "metadata": {},
   "source": [
    "![assets/imgs/simple_rag__similarity_search.png](assets/imgs/simple_rag__similarity_search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bfba0fe9-7bd2-4ec5-90a4-f22ea3ac8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_docs = vectorstore.similarity_search_with_relevance_scores(question, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40e861be-7551-446c-8a7c-ca9bf0aec1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='alongside their client project.\\n> TMC supports with a physical lab and possibly even with financial resources.\\n> The ownership of the innovation remains of our employeneurs; TMC has no interest in\\nit.\\nBUILDING THE FUTURE IN\\nThe Entrepreneurial Lab\\nThis is the place where you can let your technical dreams come true. If you\\nhave a groundbreaking idea for a new innovation, you can initiate your\\nproject and take it from concept to reality with the support of a vibrant\\ncommunity of like-minded innovators.\\x00 \\x00', metadata={'page': 0, 'source': './backend/tmc_tel_lab.pdf'}),\n",
       "  -111.32334465887976),\n",
       " (Document(page_content=\"Room to play\\nAt the Entrepreneurial Lab, we believe in the power of multidisciplinary collaboration and\\nthe freedom to explore. Here, diverse teams bringing together various skills to tackle the\\nmultifaceted challenges of today. It's where individual competencies unite to solve\\ncomplex problems, driving innovative solutions through collective expertise.\\nThrough hands-on, experienced-based learning, we create an environment where\\nexperimentation is encouraged, failures are embraced as opportunities for growth, and\\nbreakthroughs are celebrated.\\nWhether it's pioneering technology or disruptive healthcare solutions, the Entrepreneurial\\nLab provides the space, resources, and collaborative spirit to turn your vision into a\\nsuccessful venture.\\nHow it works\\nSounds promising, doesn't it? But how exactly does the Entrepreneurial Lab operate?\\n> Our employeneurs work on their own innovation if they desire, but they do this\\nalongside their client project.\", metadata={'page': 0, 'source': './backend/tmc_tel_lab.pdf'}),\n",
       "  -139.5120848479825)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf9e2e-329e-4c35-8f05-2dceea717daa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combine Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ae98a-a4fc-4f20-95ce-c929d1579f59",
   "metadata": {},
   "source": [
    "![assets/imgs/simple_rag__combine_context.png](assets/imgs/simple_rag__combine_context.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d17745b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4461264-5cac-479a-917c-9bf589826da4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOllama(model=OLLAMA_LARGE_LANGUAGE_MODEL, base_url=OLLAMA_SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5985e-4131-446f-b4e5-baa57ec1929b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592a9c4-22f4-48d8-abcd-56711ed7d6ae",
   "metadata": {},
   "source": [
    "![assets/imgs/simple_rag__combine_context.png](assets/imgs/simple_rag__response_generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ed062-f241-4f7a-9b95-f94235645172",
   "metadata": {},
   "source": [
    "[RAG chains](https://python.langchain.com/docs/expression_language/get_started#rag-search-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the provided context, the TMC Entrepreneurial Lab is a facility or program that supports innovative projects initiated by its client project teams, which include employeneurs (self-employed professionals who work within an organization like TMC). The lab provides a physical space (a \"vibrant community of like-minded innovators\") where these teams can develop and build their ideas from conception to reality. It offers both the necessary infrastructure (a lab) and possibly financial support. Notably, TMC does not claim ownership over the innovations that are developed within its Entrepreneurial Lab; the intellectual property remains with the employeneurs who initiate these projects. The lab\\'s purpose is to help build the future by fostering technological advancements and entrepreneurial spirit.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoking the RAG chain\n",
    "response = chain.invoke({\"context\":returned_doc[0], \"question\":question})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c9497",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the `chain.invoke()` example above we used directly the result output of a similarity search of the vector database. Langchain has a better approach for this via retrievers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a51dc-ea11-46b2-b8dc-272bfca63221",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e03a2ff9-2343-4047-bb07-0a3cd4de73bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# here we create a retriever from the vectorstore which can perform similarity search and returns one document\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1}, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75d07a22-8234-40fa-973a-b5a84ec14023",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, which includes \\'Chroma\\' and \\'OllamaEmbeddings\\' within tags and a reference to a `Chroma` object from the `langchain_community.vectorstores` module, it seems like you are referring to a system or application that uses Chroma, which is a vector store for similarity retrieval, possibly in the context of a larger system or service named \"TMC Entrepreneurial Lab.\"\\n\\nHowever, without additional context specific to \"TMC Entrepreneurial Lab,\" it\\'s not possible to provide a definitive answer about what it is. The term \"TMC Entrepreneurial Lab\" does not directly relate to the provided code or tags. It could be a separate entity, a project, or a program that utilizes similar technologies for search or data retrieval tasks, potentially leveraging vector embeddings (like those from \\'OllamaEmbeddings\\') stored in a Chroma vector store.\\n\\nIf \"TMC Entrepreneurial Lab\" is indeed a specific initiative, organization, or product that uses these technologies, you would need to consult documentation or resources specific to that entity to understand its purpose and functions. It could be an incubator for startups, a research lab focused on entrepreneurship, or any other initiative that employs such technology for its operations, such as to analyze startup pitches, market trends, or other data relevant to entrepreneurship using vector embeddings and similarity search capabilities.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with this retreiver the context (relevant split) is directry passed to the question addressing the LLM.\n",
    "response = chain.invoke({\"context\":retriever,\"question\":question})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748826de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HandsOn --- Answer not in Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087aad15",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> What happens if the answer is not in the splits of any retreived document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b60696b2-067e-481d-a147-37032051e5f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A large language model (LLM) is an artificial intelligence system designed to understand, generate, and work with human language at a level that can often seem coherent and contextually relevant. These models are typically trained on vast amounts of text data and have the capability to perform a wide range of language-related tasks, such as translation, question answering, summarization, and more.\\n\\nThe term \"large\" refers to the model\\'s size in terms of the number of parameters it containsâ€”ranging from millions to hundreds of billions. Examples of large language models include OpenAI\\'s GPT (Generative Pre-trained Transformer) series, Google\\'s BERT (Bidirectional Encoder Representations from Transformers), and others like T5 (Text-to-Text Transfer Transformer).\\n\\nThe context you provided mentions \\'Chroma\\' and \\'OllamaEmbeddings\\' within a technical setting that suggests the use of these tools or libraries for handling vector data. Chroma is a vector store that allows for efficient retrieval of vectors (which can be used to represent text in high-dimensional space), and OllamaEmbeddings seems to be a related entity, possibly referring to a method or model for generating embeddings (vector representations) that could be stored in Chroma. These tools are often used in conjunction with large language models for various natural language processing tasks.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"context\":retriever,\"question\":\"What is a large language model?\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9fb95-db25-4a0d-ad21-2462e84c6206",
   "metadata": {},
   "source": [
    "## HandsOn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ec39d-8bff-4c49-8582-268921e7854d",
   "metadata": {},
   "source": [
    "1. Rerun the overall app using a question which relates to the [website](https://www.themembercompany.com/nl/employeneurship). You first will have to load the website via WebBaseLoader.\n",
    "2. Directly ask the llm something ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35599efd-fc0c-4daf-955a-931e8cfd3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOllama(model=OLLAMA_LARGE_LANGUAGE_MODEL, base_url=OLLAMA_SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad44b74d-48af-4bfe-a0f9-ef6eeae0313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(\"Tell me a joke about the weather.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a33ae01-b7d4-4e9e-9240-7c00d9f53bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s a light-hearted weather joke for you:\\n\\nWhy don\\'t clouds ever get cold?\\n\\nBecause most of their sweat evaporates before it reaches their feet! (A play on words with \"evaporation\" and \"sweat\")\\n\\nRemember, jokes about the weather are often as changeable as the forecast itself!'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f197a-7dc4-42c5-a301-82634932b53e",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff57604-cb72-43c9-ba68-3131a2f0b282",
   "metadata": {},
   "source": [
    "Implementation of Simple Retrieval-Augmented Generation (RAG) from Scratch\n",
    "- **Ingestion Phase**:\n",
    "    - **Load**: Loading documents into the system.\n",
    "    - **Split**: Splitting documents into manageable chunks.\n",
    "    - **Embed**: Embedding document chunks into vector representations.\n",
    "    - **Store**: Storing the embedded documents in a vector store.\n",
    "- **Similarity Search**: Searching for relevant documents using embedded query.\n",
    "- **Combine Context**: Combining the retrieved document context with the query.\n",
    "- **Response Generation**: Generating the final response using a Large Language Model (LLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb00b8-cb17-4ae5-aef4-e0be6e89c070",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0a9c4-076a-4da9-bde9-f51cb30ee87a",
   "metadata": {},
   "source": [
    "There is plenty more to discover at [langchain's](https://github.com/langchain-ai) and many other websites! Especially check out: [YouTube](https://www.youtube.com/watch?v=sVcwVQRHIc8&ab_channel=freeCodeCamp.org) and [Github](https://github.com/langchain-ai/rag-from-scratch/tree/main). Here an overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33306e55-0ce9-4db7-ba48-5355311b5cd9",
   "metadata": {},
   "source": [
    "![assets/imgs/langchain_rag_overview.png](assets/imgs/langchain_rag_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19445455-22ab-451a-a28c-e52c52112235",
   "metadata": {},
   "source": [
    "## Thank You"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9dc9a-78d4-4088-87aa-17a972db96bf",
   "metadata": {},
   "source": [
    "- **Technical Support**: Milan and Raul\n",
    "- **Organisational Support**: Marlies, Wendy, and Varsha\n",
    "- **Motivational Support**: TMChampionship/ TEL/ Pepijn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed862a6c-8654-4e9c-beaa-a600d85b9496",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e65d82-c245-44d9-bae6-c9fadc326856",
   "metadata": {},
   "source": [
    "![assets/imgs/ai_workshop_tmc__feedback.png](assets/imgs/ai_workshop_tmc__feedback.png)\n",
    "\n",
    "https://forms.office.com/e/CwRvint3LY?origin=lprLink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fdf7bb-0415-4bec-b3b9-412f09e1eba6",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4398734-192f-4ee4-bf37-47bbc7aec96f",
   "metadata": {},
   "source": [
    "- **credits**: this notebook heavily borrows from langchain's [rag_from_scratch_1_to_4.ipynb](\"https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8a69a-5f46-4672-97ac-24ae362f04b5",
   "metadata": {},
   "source": [
    "* **GPT**: Generative Pre-trained Transformer, a type of language model developed by OpenAI that generates human-like text using transformer architecture.\n",
    "* **LLM**: Large Language Model, a machine learning model trained on vast amounts of text data to understand and generate human language.\n",
    "* **Transformer**: Deep learning model using attention mechanism for context understanding and parallel processing, introduced in the \"Attention is All You Need\" paper.\n",
    "* **Embedding Models**: Convert text to vector representations (e.g., BERT).\n",
    "* **Generation Models**: Generate text from prompts (e.g., GPT-3).\n",
    "* **Softmax Function**: Converts values to probabilities, used in classification models.\n",
    "* **Fine-Tune vs. Retrieval \"Augmented Generation**\n",
    "    * **Fine-Tuning an LLM**: Adapts model to specific tasks using labeled data.\n",
    "    * **RAG (Retrieval-Augmented Generation)**: Combines retrieval with generation for context-specific responses.\n",
    "* **Micro Timeline**\n",
    "    * **2017**: \"Attention is All You Need\" paper.\n",
    "    * **2018**: BERT, GPT-2\n",
    "    * **2020**: GPT-3.\n",
    "* **Quantization**: Reduces precision of model parameters.\n",
    "    * **Benefits**: Smaller size, faster inference, lower power consumption.\n",
    "    * **Types**: Static, Dynamic, Quantization-Aware Training.\n",
    "    * **Challenges**: Accuracy loss, hardware support needed.\n",
    "* **Not all LLMs are GPTs**: Other models include BERT, T5, XLNet, RoBERTa.\n",
    "* **Not all LLMs use transformers**: Other architectures include RNNs, CNNs, MoE, Memory-Augmented Networks."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
